{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P7 Chapter 1 - The Human Data digital Toolkit (HDDT) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thesis Chapter 6 Sections 6.16 and 6.17 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File name: jnb_hddt_intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Links to chapter sections # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Jump to: |\n",
    "|---|\n",
    "|[My PhD project](#1)|\n",
    "|[HDDT Objective](#2)|\n",
    "|[Datasets](#3)|\n",
    "|[Data Pipeline](#4)|\n",
    "|[SQLite Database](#5)|\n",
    "|[Architecture](#6)|\n",
    "|[Big Data](#7)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "\n",
    "# 1.2 The data and model objectives #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data: ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My own research, in collaboration with others, has revealed the extensive social connectivity between the roughly 3000 members of a ‘Quaker Led Group (QLG). The QLG's activities are spread across five organisations in Britain active between 1830 and 1870, which the Quaker members helped to set up and staff. I call these five organisations, the ‘Centres for the Emergence of Discipline of Anthropology in Britain’ (CEDA).Amongst the 3000 members of the CEDA are approximatley 600 Quakers, approximately 20% of the membership.\n",
    "\n",
    "The CEDA comprises:\n",
    "\n",
    "- The Quaker Committee on the Aborigines, QCA - (1831 – 1846)\n",
    "- The Aborigines Protection Society, APS - (1837 – 1848)\n",
    "- The Ethnological Society of London, ESL - (1843 – 1848)\n",
    "- The Anthropological Society of London, ASL - (1861 – 1869)\n",
    "- The Anthropological Institute, AI – (1871). A merger of the ESL and ASL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The HDDT requirements ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Able to handle the data (quantity and quality)\n",
    "- User interventions\n",
    "- Open source\n",
    "- In common use\n",
    "- Ease of Interoperability\n",
    "- Free online learning aids\n",
    "- Supported by University of Birmingham\n",
    "- Independent Research Group friendly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model: ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have designed, built a suite of open-source relational database technologies and digital analytic tools to visualise and scrutinise the entire CEDA community over the 40 years of their collective action from their beginnings up to the formation of the Royal Anthropological Institute (1830-1870). I have identified the Quakers amongst them so that the community can be explored at both whole group and Quaker members levels. I am able to model the prosopographical relationships between the individual members of the CEDA both statically and dynamically (through time). I can analyse bipartite relationships for common attributes: kinship (Quakers only), education, occupations, locations and organisations.I have buit a Human Data Digital Toolkit (HDDT)to collect, clean, manage and present the data. The model and data have been designed to answer three questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can the model reveal the connectivity between the CEDA memberships and the four organisations that began with a concern for the plight of aborigines and led to the establishment of the discipline of anthropology in Britain 1830 - 1870? This question is important because it resolves current uncertainty over the origins of the discipline of anthropology in Britain and the extent of Quaker involvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can the model seamlesssly facilitate an examination of Quaker to Quaker relationships, Quaker roles in the CEDA at the individual and at the individual society and whole group levels? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can the model reveal the key networking role played by the Quaker Thomas Hodgkin MD (1798 – 1866) from the beginnings of the CEDA in 1830 and up to his death in 1866?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "\n",
    "To create containers for the data collected in csv sheets from several sources and in different arrangements. To facilitate data combining and cleaning. To establish an auditable data pipeline. To hold the data in an sql database.  The model data  can then be analysed and visualised by using Gephi which is a compatible open source software package. The HDDT is designed to enable human prosopographical ordered data to be surveyed, much as an archaeologist might use a variety of technologies to survey a territorial site of historical interest. \n",
    "\n",
    "The HDDT is a new approach to the Digital Humanities. (1) in is exclusive concentrtion on Evidence Basded Prosopographical data  using open source data science techniques readily available to the lone researcher and capable of execution on an average desktop computer. The objective is to facilitate the study large sets of prosopographical data (of variable quality and quantity) either as individuals or groups of persons and embedded sub-groups of persons.\n",
    "\n",
    "Exponential population growth in the nineteenth century combined with a cultural, disciplined  and extensive interest in collecting, cataloguing and preserving historial artefacts and manuscripts, offers to the researcher an opportunity to study individuals, communities and whole sections of societies en masse.\n",
    "\n",
    "The archives of the nineteenth century are often very large, too big to be surveyed only by the naked eye. (and are immense when viewed collectively). From a prosopographical point of view they offer a rich source of prosopographical data, often organised as metadata (catalogued and frequently cross-referenced, if poorly referenced to primary sources).\n",
    "\n",
    "The HDDT facilitates the study of prosopographical data in itself, it is purposely free of narratology and interpretation. It, and other tools like it are essentil to all researchers wishing to underdand the peoples of the past and their relationships.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 HTTD technical Objective #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diggers.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archival data can now be surveyed much as an archaeological site can - by using technology ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Contributing datasets #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The HDDT integrates ordered datasets from a variety of sources to create one SQLite HTTD database ##\n",
    "\n",
    "Historians can create a bespoke database taking data from multiple sources using the HDDT. This project takes data from:\n",
    "\n",
    "| Source |Records |\n",
    "| --- |---|\n",
    "| Royal Anthropological Insitiute (RAI)| 2260 |\n",
    "| Quaker Family History Society (QFHS) |593|\n",
    "| Independent research at RAI |1171|\n",
    "| Independent research at Friends House Quaker Archive, London |30|\n",
    "| total records | 3095 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Dataset variability #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Component datasets can be 'Complete', 'Incomplete' or 'Irregular' as long as the contributing datasets consist of records where at least one column is shared. In this Project person_name was common to all datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A 'complete' dataset ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would be one like this, where all of the data is contained witin a perfect rectangular block of cells ('containers') and every container contains only one data item and every data item can be located by the coordinates 'Row n, Column n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An 'incomplete' dataset ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When historical data is used often some data is missing (it can be permanently lost). The HDDT is able to accept 'Incomplete' datasets. The HDDT does not lose functionality because of the incomplete nature of much historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data_3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An 'irregular' dataset ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HDDT has been designed to accept Irregular datasets. The surviving evidence of past lives is not only often Incomplete, it is frequently Irregular. An additional complication arsises when data from multiple sourcesmust be combined into a single dataset. Here it is likely that multiple donor datasets will have have different dimensions. This arises because either the data in itself is intrinsically different or because different data collectors have compiled data at different times and to different standards or simply prefer different collecting methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the HDDT a qualifying contributing dataset is a data set of any dimensions, complete, incomplete or irregular. The only requirement is that all contributing datasets must consist of prosopographical data and with the key field as PERSON-NAME."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6 Data pipeline #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the HDDT data transits through a data pipeline with each component of the pipeline managed by a different technology. Great care was taken to ensure the the technological array comprised of open source technologies, fully integratable (losslessly) and widely supported and in wide use in the academic community. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pipeline.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.7 Model output - Data visualisation criteria #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to answer the three questions set the selected visualisation technology needed to be able to produce an affiliation network for social network analysis (a bipartite graph). Additionally it needed to be able to present data statically, where all 40 years data are compressed into one visualisation. This would enable the full extent of the network examined to be seen as a single image. Additionally, because the network evolves and changes over time, it was desirable that the technology handle dynamic data, where the network can be visualised progressively one year at a time from 1830 to 1870. Gephi met the project criteria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.8 Integrated HTTD SQLite database - 3095 persons #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Relationship Diagram (ERD) ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ERD.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "At the heart of the HDDT (and the SQLite database) is the 'person table', this holds the data (attributes) unique to each person. All contributing datasets have one or more columns containing person names. Further columns often contain attribut data. Some attribute data may be recorded in sevearl donor datasets. Conflicts between dataset Person Name's are resolved by first choosing to accept the 'RAI dataset' as the 'Authority Index'. Matching names across donor datasets was done by hand in the CSV sheets. This is because (1) person name records from datasets other then the deemed (RAI) dataset were small in number, (2) because name matching requires judgement (names ofthen repeat in familiies and their might be little attributive data in common between the other donor data set and the RAI dataset and (3) because human matching is common practise in genealogy systems, even if algorithms are used to find possible matches. The finding of possible matches in this project was performed most efficiently by the human eye. (A move to the digital should not needlessly repalce the human). \n",
    "\n",
    "Tables of data items shared amongst persons (such as 'occupation', 'location', 'societies', 'clubs') are linked to the person table by m2m tables.\n",
    "\n",
    "There are also person_person tables to capture family relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "\n",
    "# 1.9 HDDT Design Architecture #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following packages are required to make the HDDT:\n",
    "\n",
    "| package | Use | \n",
    "| --- | --- |\n",
    "| GitHub | for version control and sharing | \n",
    "| SQL | the database |\n",
    "| VSC | building the database |\n",
    "| VSC | version control interface to Git |\n",
    "| DBeaver | data cleaning, data management and ananysis |\n",
    "| Jupyter Notebook | data analysis |\n",
    "| Gephi | data visualisation | \n",
    "\n",
    "\n",
    "They were chosen because they are universal, popular, open-source and suitable for handling historical ordered data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.10 Project containers contain all required resources #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data management needs caeful consideration and design. The HDDT uses the cocept of project containers where every container is set up and initialised as a GitHub repo. Then a Jupyter Notebook is created in the same container. All resources needed for a project are then copied from master containers (such as the template CSV files and dataframes in the ceda/database/views container.\n",
    "\n",
    "Gexf graph files and gexf project files also are set up and saved in each container.\n",
    "\n",
    "Relative links can then be used and their integrity preserved.\n",
    "\n",
    "GitGub can also be used for version control providing an audit trail of changes, additions and deletions to the HDDT container system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.11 Github #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire HDDT project, its description, structure, organisation and resources are contained in one GitHub account:\n",
    "\n",
    "https://github.com/KelvinBeerJones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"git.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.12 SQLite recommendations #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQLite is a C-language library that implements a small, fast, self-contained, high-reliability, full-featured, SQL database engine. SQLite is the most used database engine in the world. SQLite is built into all mobile phones and most computers and comes bundled inside countless other applications that people use every day (https://www.sqlite.org/index.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQLite is a C-language library that implements a small, fast, self-contained, high-reliability, full-featured, SQL database engine. SQLite is the most used database engine in the world. SQLite is built into all mobile phones and most computers and comes bundled inside countless other applications that people use every day. The SQLite file format is stable, cross-platform, and backwards compatible and the developers pledge to keep it that way through the year 2050. SQLite database files are commonly used as containers to transfer rich content between systems and as a long-term archival format for data. There are over 1 trillion SQLite databases in active use. SQLite source code is in the public-domain and is free to everyone to use for any purpose. https://www.sqlite.org/about.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the two decades following its initial release, SQLite has become the most widely deployed database engine in existence. Today, SQLite is found in nearly every smartphone, computer, web browser, television, and automobile. Several factors are likely responsible for its ubiquity, including its in-process design, standalone codebase, extensive test suite, and cross-platform file format. While it supports complex analytical queries, SQLite is primarily designed for fast online transaction processing (OLTP), employing row-oriented execution and a B-tree storage format. However, fueled by the rise of edge computing and data science, there is a growing need for efficient in-process online analytical processing (OLAP). DuckDB, a database engine nicknamed \"the SQLite for analytics\", has recently emerged to meet this demand. While DuckDB has shown strong performance on OLAP benchmarks, it is unclear how SQLite compares. Furthermore, we are aware of no work that attempts to identify root causes for SQLite's performance behavior on OLAP workloads. In this paper, we discuss SQLite in the context of this changing workload landscape. We describe how SQLite\n",
    "evolved from its humble beginnings to the full-featured database engine it is today. We evaluate the performance of modern SQLite on three benchmarks, each representing a different flavor of in-process data management, including transactional, analytical, and blob processing. We delve into analytical data processing on SQLite, identifying key bottlenecks and weighing potential solutions. As a result of our optimizations, SQLite is now up to 4.2X faster on SSB. Finally, we discuss the future of SQLite, envisioning how it will evolve to meet new demands and challenges. 'Sqlite: past, present, and future' Gaffney, Kevin P, Prammer, Martin Brasfield, Larry Hipp, D Richard Kennedy, Dan Patel, Jignesh M (Gaffney et al. 2022, 3535)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.13 VSC recommendations #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual Studio Code is a lightweight but powerful source code editor which runs on your desktop and is available for Windows, macOS and Linux. It comes with built-in support for JavaScript, TypeScript and Node.js and has a rich ecosystem of extensions for other languages (such as C++, C#, Java, Python, PHP, Go) and runtimes (such as .NET and Unity)(https://code.visualstudio.com/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‘Visual Studio Code, commonly referred to as VS Code,is an integrated development environment developed by Microsoft for Windows, Linux, macOS and web browsers. Features include support for debugging, syntax highlighting, intelligent code completion, snippets, code refactoring, and embedded version control with Git. Users can change the theme, keyboard shortcuts, preferences, and install extensions that add functionality.\n",
    "Visual Studio Code is proprietary software released under the \"Microsoft Software License\", but based on the MIT licensed program named \"Visual Studio Code — Open Source\" (also known as \"Code — OSS\"), also created by Microsoft and available through GitHub.\n",
    "In the 2024 Stack Overflow Developer Survey, out of 58,121 responses, 73.6% of respondents reported using Visual Studio Code, more than twice the percentage of respondents who reported using its nearest text editor and/or IDE alternative, Visual Studio. https://en.wikipedia.org/wiki/Visual_Studio_Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‘Microsoft develops the free code editor Visual Studio Code (VSCode), that is used by more than 11 million users. In the StackOverflow developer survey from 2019, about 50% of the participants stated that they use VSCode, showing how popular this\n",
    "editor has become. The underlying source code is considered free (as in free speech) and referred to as Code - OSS. Microsoft uses Code - OSS as a base, slightly modifies it (e.g. adds a marketplace integration for distributing plugins), and releases it with a proprietary license under the name \\Visual Studio Code\". Although this custom license makes VSCode technically speaking non-free and non-open source, other distributions of Code - OSS are free and contain substitutes for the missing features, for\n",
    "example VSCodium. Further analysis of VSCode and Code - OSS regarding aspects of FOSS (free and open source software) development can be found in the extensive preliminary study that was conducted before the project phase.’ ' Practical Study of Visual Studio Code' Michael Plainer (Plainer 2021, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"vsc.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.14 VSC (Version control interface local to online Git Repo's #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"vsc_for_git _version_control.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.15 DBeaver recommendations #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‘DBeaver is a universal database management tool for everyone who needs to work with data in a professional way. With DBeaver you are able to manipulate your data, for example, in a regular spreadsheet, create analytical reports based on records from different data storages, and export information in an appropriate format. For advanced database users, DBeaver suggests a powerful SQL-editor, plenty of administration features, abilities of data and schema migration, monitoring database connection sessions, and a lot more.\n",
    "Out-of-the box DBeaver supports more than 80 databases. Having usability as its main goal, DBeaver offers:\n",
    "- Carefully designed and implemented User Interface\n",
    "- Support of Cloud data sources\n",
    "- Support for Enterprise security standard\n",
    "- Capability to work with various extensions for integration with Excel, Git, and others.\n",
    "- Great number of features\n",
    "- Multiplatform support’\n",
    "https://github.com/dbeaver/dbeaver/wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBeaver is a SQL client software application and a database administration tool. For relational databases it uses the JDBC application programming interface (API) to interact with databases via a JDBC driver. For other databases (NoSQL) it uses proprietary database drivers. It provides an editor that supports code completion and syntax highlighting. It provides a plug-in architecture (based on the Eclipse plugins architecture) that allows users to modify much of the application's behavior to provide database-specific functionality or features that are database-independent. It is written in Java and based on the Eclipse platform. The community edition (CE) of DBeaver is a free and open source software that is distributed under the Apache License. A closed-source enterprise edition of DBeaver is distributed under a commercial license. https://en.wikipedia.org/wiki/DBeaver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Universal Database Tool\n",
    "\n",
    "DBeaver is a free multi-platform database tool for developers, database administrators, analysts and all people who need to work with databases. Supports all popular databases: MySQL, PostgreSQL, SQLite, Oracle, DB2, SQL Server, Sybase, MS Access, Teradata, Firebird, Apache Hive, Phoenix, Presto, etc.(https://dbeaver.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dbeaver.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.16 Jupyter Notebook recommendations #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning. (https://jupyter.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‘The Jupyter notebook is an open-source, browser-based tool functioning as a virtual lab notebook to support workflows, code, data, and visualizations detailing the research process. It is machine and human-readable, which facilitates interoperability and scholarly communication. These notebooks can live in online repositories and provide connections to research objects such as datasets, code, methods documents, workflows, and publications that reside elsewhere. Jupyter notebooks are one means to make science more open. Their relevance to the JCDL community lies in their interaction with multiple components of\n",
    "digital library infrastructure such as digital identifiers, persistence mechanisms, version control, datasets, documentation, software, and publications. Our poster examines how Jupyter notebooks embody the FAIR (Findable, Accessible, Interoperable, Reusable) principles for digital objects and assess their utility as viable tools for scholarly communication.’ 'Using the Jupyter notebook as a tool for open science: An empirical study' Randles Bernadette M, Pasquetto Irene V, Golshan Milena S,\n",
    "Borgman Christine L. (Randles et al. 2017, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"jnb.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.17 Gephi recommends #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‘Gephi is an open source software for graph and network analysis. It uses a 3D render engine to display large networks in real-time and to speed up the exploration. A flexible and multi-task architecture brings new possibilities to work with complex data sets and produce valuable visual results. We present several key features of Gephi in the context of interactive exploration and interpretation of networks. It provides easy and broad access to network data and allows for spatializing, filtering, navigating, manipulating and clustering. Finally, by presenting dynamic features of Gephi, we highlight key aspects of dynamic network visualization.’ Bastian Mathieu, Heymann Sebastien, Jacomy, Mathieu Gephi: an open source software for exploring and manipulating networks. (Bastian, Heymann, and Jacomy 2009, 361)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gephi is a tool for data analysts and scientists keen to explore and understand graphs. Like Photoshop™ but for graph data, the user interacts with the representation, manipulate the structures, shapes and colors to reveal hidden patterns. The goal is to help data analysts to make hypothesis, intuitively discover patterns, isolate structure singularities or faults during data sourcing. It is a complementary tool to traditional statistics, as visual thinking with interactive interfaces is now recognized to facilitate reasoning. This is a software for Exploratory Data Analysis, a paradigm appeared in the Visual Analytics field of research. https://gephi.org/features/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gephi is a tool for data analysts and scientists keen to explore and understand graphs. Like Photoshop™ but for graph data, the user interacts with the representation, manipulate the structures, shapes and colors to reveal hidden patterns. The goal is to help data analysts to make hypothesis, intuitively discover patterns, isolate structure singularities or faults during data sourcing. It is a complementary tool to traditional statistics, as visual thinking with interactive interfaces is now recognized to facilitate reasoning. This is a software for Exploratory Data Analysis, a paradigm appeared in the Visual Analytics field of research.(https://gephi.org/features/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"gephi.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.18 Excel recommends #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.datacamp.com/tutorial/data-cleaning-in-excel-a-beginners-guide \n",
    "\n",
    "https://cartong.pages.gitlab.cartong.org/learning-corner/en/3_nettoyage/3_3_nettoyage_donnees "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.19 References #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bastian, Mathieu, Sebastien Heymann, and Mathieu Jacomy. 2009. \"Gephi: an open source software for exploring and manipulating networks.\" Proceedings of the international AAAI conference on web and social media.\n",
    "\n",
    "Gaffney, Kevin P, Martin Prammer, Larry Brasfield, D Richard Hipp, Dan Kennedy, and Jignesh M Patel. 2022. \"Sqlite: past, present, and future.\" Proceedings of the VLDB Endowment 15 (12).\n",
    "\n",
    "Plainer, Michael. 2021. \"Practical Study of Visual Studio Code.\"\n",
    "\n",
    "Randles, Bernadette M, Irene V Pasquetto, Milena S Golshan, and Christine L Borgman. 2017. \"Using the Jupyter notebook as a tool for open science: An empirical study.\" 2017 ACM/IEEE Joint Conference on Digital Libraries (JCDL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.20  600 Quakers amongst 3000 activists for 40 years. #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"big_data.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
